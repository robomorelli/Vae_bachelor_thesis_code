{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Add, Multiply\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import layers as KL\n",
    "\n",
    "import uproot\n",
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer, LeakyReLU, ReLU\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, BaseLogger, \\\n",
    "TerminateOnNaN, Callback, ModelCheckpoint, LambdaCallback\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.constraints import max_norm\n",
    "from sklearn.externals.joblib import dump, load\n",
    "\n",
    "from numpy.random import seed\n",
    "import time\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import random as rn\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "# import ray\n",
    "# from ray.tune.integration.keras import TuneReporterCallback\n",
    "# from ray import tune\n",
    "# from ray.tune.schedulers import AsyncHyperBandScheduler, ASHAScheduler, PopulationBasedTraining\n",
    "import random\n",
    "import csv\n",
    "\n",
    "\n",
    "from config import *\n",
    "from vae_utility import *\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "np.random.seed(42)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "rn.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "back = np.load(train_val_test + '/background_train.npy')\n",
    "back_val = np.load(train_val_test + '/background_val.npy')\n",
    "\n",
    "#remove weights from the train-val data:\n",
    "train = back[:,:-1]\n",
    "val = back_val[:,:-1]\n",
    "\n",
    "components_dict = {\n",
    "    'met':1,\n",
    "    'mt':1,\n",
    "    'mbb':2,\n",
    "    'mct2':1,\n",
    "    'mlb1':1,\n",
    "    'lep1Pt':1,\n",
    "    'nJet30':1,\n",
    "    'nBJet30_MV2c10':1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_components = []\n",
    "\n",
    "for k,v in components_dict.items():\n",
    "    if components_dict[k] != 0:\n",
    "        selected_components.append(k)\n",
    "\n",
    "weights = []\n",
    "\n",
    "for k,v in components_dict.items():\n",
    "    weights.append(v)\n",
    "\n",
    "selected_idx = [cols.index(component) for component in selected_components]\n",
    "\n",
    "if 0 in weights:\n",
    "    ind_w = [x if x != 0 else 1 for x in weights]\n",
    "else:\n",
    "    ind_w = weights\n",
    "\n",
    "w = weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = train.shape[1]\n",
    "Nf_lognorm = 6\n",
    "Nf_PDgauss = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\" model bump_feat_0_1_2_3_4_5_6_7_ld_4 with selected component: ['met', 'mt', 'mbb', 'mct2', 'mlb1', 'lep1Pt', 'nJet30', 'nBJet30_MV2c10'] and weight [1, 1, 2, 1, 1, 1, 1, 1] in loss, individual weights[1, 1, 2, 1, 1, 1, 1, 1]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# name=name\n",
    "# w=weights\n",
    "# ind_w = individual_weights\n",
    "intermediate_dim=50\n",
    "act_fun='relu'\n",
    "latent_dim=4\n",
    "kernel_max_norm=500\n",
    "lr=0.003\n",
    "epochs=2000 \n",
    "weight_KL_loss=0.6\n",
    "batch_size=200\n",
    "\n",
    "name = 'bump_feat_{}_ld_{}'.format('_'.join([str(x) for x in selected_idx]), latent_dim)\n",
    "\n",
    "try:\n",
    "    os.makedirs(model_results_bump_single + name)\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "with open(model_results_bump_single + name + '/' + 'comps_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(components_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "\n",
    "print(\"\\x1b[31m\\\" model {} with selected component: {} and weight {} in loss, individual weights{}\"\"\\x1b[0m\"\\\n",
    "      .format(name, selected_components, weights, ind_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_loss_forVAE(mu, sigma, mu_prior, sigma_prior):\n",
    "    kl_loss = K.tf.multiply(K.square(sigma), K.square(sigma_prior))\n",
    "    kl_loss += K.square(K.tf.divide(mu_prior - mu, sigma_prior))\n",
    "    kl_loss += K.log(K.tf.divide(sigma_prior, sigma)) -1\n",
    "    return 0.5 * K.sum(kl_loss, axis=-1)\n",
    "\n",
    "#######################################################################\n",
    "def RecoProb_forVAE(x, par1, par2, par3, w):\n",
    "\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    if Nf_lognorm != 0:\n",
    "\n",
    "        for i in range(Nf_lognorm):\n",
    "\n",
    "            #Log-Normal distributed variables\n",
    "            mu = par1[:,i:i+1]\n",
    "            sigma = par2[:,i:i+1]\n",
    "            fraction = par3[:,i:i+1]\n",
    "            x_clipped = K.clip(x[:,i:i+1], clip_x_to0, 1e8)\n",
    "            single_NLL = K.tf.where(K.less(x[:,i:i+1], clip_x_to0),\n",
    "                                    -K.log(fraction),\n",
    "                                        -K.log(1-fraction)\n",
    "                                        + K.log(sigma)\n",
    "                                        + K.log(x_clipped)\n",
    "                                        + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "            nll_loss += K.sum(w[i]*single_NLL, axis=-1)\n",
    "\n",
    "        N += Nf_lognorm\n",
    "\n",
    "    if Nf_PDgauss != 0:\n",
    "\n",
    "        for i in range(N, N+Nf_PDgauss):\n",
    "\n",
    "            mu = par1[:,i:i+1]\n",
    "            sigma = par2[:,i:i+1]\n",
    "            norm_xp = K.tf.divide(x[:,i:i+1] + 0.5 - mu, sigma)\n",
    "            norm_xm = K.tf.divide(x[:,i:i+1] - 0.5 - mu, sigma)\n",
    "            sqrt2 = 1.4142135624\n",
    "            single_LL = 0.5*(K.tf.erf(norm_xp/sqrt2) - K.tf.erf(norm_xm/sqrt2))\n",
    "\n",
    "            norm_0 = K.tf.divide(-0.5 - mu, sigma)\n",
    "            aNorm = 1 + 0.5*(1 + K.tf.erf(norm_0/sqrt2))\n",
    "            single_NLL = -K.log(K.clip(single_LL, 1e-10, 1e40)) -K.log(aNorm)\n",
    "\n",
    "            nll_loss += K.sum(w[i]*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_1(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,:1]\n",
    "    sigma = par2[:,:1]\n",
    "    fraction = par3[:,:1]\n",
    "    x_clipped = K.clip(x[:,:1], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,:1], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_2(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,1:2]\n",
    "    sigma = par2[:,1:2]\n",
    "    fraction = par3[:,1:2]\n",
    "    x_clipped = K.clip(x[:,1:2], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,1:2], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_3(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,2:3]\n",
    "    sigma = par2[:,2:3]\n",
    "    fraction = par3[:,2:3]\n",
    "    x_clipped = K.clip(x[:,2:3], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,2:3], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_4(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,3:4]\n",
    "    sigma = par2[:,3:4]\n",
    "    fraction = par3[:,3:4]\n",
    "    x_clipped = K.clip(x[:,3:4], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,3:4], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_5(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,4:5]\n",
    "    sigma = par2[:,4:5]\n",
    "    fraction = par3[:,4:5]\n",
    "    x_clipped = K.clip(x[:,4:5], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,4:5], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def IndividualRecoProb_forVAE_lognorm_6(x, par1, par2, par3, w):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,5:6]\n",
    "    sigma = par2[:,5:6]\n",
    "    fraction = par3[:,5:6]\n",
    "    x_clipped = K.clip(x[:,5:6], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,5:6], clip_x_to0),\n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma)\n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma)))\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "\n",
    "\n",
    "def individualRecoProb_forVAE_discrete_7(x, par1, par2, w):\n",
    "    N = Nf_lognorm\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,6:7]\n",
    "    sigma = par2[:,6:7]\n",
    "    norm_xp = K.tf.divide(x[:,6:7] + 0.5 - mu, sigma)\n",
    "    norm_xm = K.tf.divide(x[:,6:7] - 0.5 - mu, sigma)\n",
    "    sqrt2 = 1.4142135624\n",
    "    single_LL = 0.5*(K.tf.erf(norm_xp/sqrt2) - K.tf.erf(norm_xm/sqrt2))\n",
    "\n",
    "    norm_0 = K.tf.divide(-0.5 - mu, sigma)\n",
    "    aNorm = 1 + 0.5*(1 + K.tf.erf(norm_0/sqrt2))\n",
    "    single_NLL = -K.log(K.clip(single_LL, 1e-10, 1e40)) -K.log(aNorm)\n",
    "\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss\n",
    "\n",
    "def individualRecoProb_forVAE_discrete_8(x, par1, par2, w):\n",
    "    N = Nf_lognorm\n",
    "    nll_loss = 0\n",
    "\n",
    "    mu = par1[:,7:8]\n",
    "    sigma = par2[:,7:8]\n",
    "    norm_xp = K.tf.divide(x[:,7:8] + 0.5 - mu, sigma)\n",
    "    norm_xm = K.tf.divide(x[:,7:8] - 0.5 - mu, sigma)\n",
    "    sqrt2 = 1.4142135624\n",
    "    single_LL = 0.5*(K.tf.erf(norm_xp/sqrt2) - K.tf.erf(norm_xm/sqrt2))\n",
    "\n",
    "    norm_0 = K.tf.divide(-0.5 - mu, sigma)\n",
    "    aNorm = 1 + 0.5*(1 + K.tf.erf(norm_0/sqrt2))\n",
    "    single_NLL = -K.log(K.clip(single_LL, 1e-10, 1e40)) -K.log(aNorm)\n",
    "\n",
    "    nll_loss += K.sum(w*single_NLL, axis=-1)\n",
    "\n",
    "    return nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomKLLossLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomKLLossLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mu, sigma, mu_prior, sigma_prior = inputs\n",
    "        return KL_loss_forVAE(mu, sigma, mu_prior, sigma_prior)\n",
    "\n",
    "\n",
    "class CustomRecoProbLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomRecoProbLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return RecoProb_forVAE(x, par1, par2, par3, w = w)\n",
    "\n",
    "#################################################################################################Ã \n",
    "class CustomIndividualLogNorLayer_1(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_1, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_1(x, par1, par2, par3, w = ind_w[0])\n",
    "\n",
    "class CustomIndividualLogNorLayer_2(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_2, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_2(x, par1, par2, par3, w = ind_w[1])\n",
    "\n",
    "\n",
    "class CustomIndividualLogNorLayer_3(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_3, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_3(x, par1, par2, par3, w = ind_w[2])\n",
    "\n",
    "class CustomIndividualLogNorLayer_4(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_4, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_4(x, par1, par2, par3, w = ind_w[3])\n",
    "\n",
    "class CustomIndividualLogNorLayer_5(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_5, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_5(x, par1, par2, par3, w = ind_w[4])\n",
    "\n",
    "class CustomIndividualLogNorLayer_6(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualLogNorLayer_6, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2, par3 = inputs\n",
    "        return IndividualRecoProb_forVAE_lognorm_6(x, par1, par2, par3, w = ind_w[5])\n",
    "\n",
    "class CustomIndividualTruGauLayer_7(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualTruGauLayer_7, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2 = inputs\n",
    "        return individualRecoProb_forVAE_discrete_7(x, par1, par2, w = ind_w[6])\n",
    "\n",
    "class CustomIndividualTruGauLayer_8(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomIndividualTruGauLayer_8, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, par1, par2 = inputs\n",
    "        return individualRecoProb_forVAE_discrete_8(x, par1, par2, w = ind_w[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveAutoencoder(ModelCheckpoint):\n",
    "    def __init__(self, filepath, monitor='val_loss', verbose=0,\n",
    "         save_best_only=False, save_weights_only=False, mode='auto', period=1):\n",
    "\n",
    "        super(ModelCheckpoint, self).__init__()\n",
    "\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.filepath = filepath\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.period = period\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "\n",
    "            warnings.warn('ModelCheckpoint mode %s is unknown, '\n",
    "                      'fallback to auto mode.' % (mode),\n",
    "                      RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                    self.monitor_op = np.greater\n",
    "                    self.best = -np.Inf\n",
    "            else:\n",
    "                    self.monitor_op = np.less\n",
    "                    self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "\n",
    "        autoencoder = Model(inputs=self.model.input,\n",
    "                                 outputs=[self.model.get_layer('Output_par1').output,\n",
    "                                         self.model.get_layer('Output_par2').output,\n",
    "                                          self.model.get_layer('Output_par3').output])\n",
    "\n",
    "\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current is None:\n",
    "                    warnings.warn('Can save best model only with %s available, '\n",
    "                                  'skipping.' % (self.monitor), RuntimeWarning)\n",
    "                else:\n",
    "                    if self.monitor_op(current, self.best):\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                                  ' saving model to %s'\n",
    "                                  % (epoch + 1, self.monitor, self.best,\n",
    "                                     current, filepath))\n",
    "                        self.best = current\n",
    "                        if self.save_weights_only:\n",
    "                            autoencoder.save_weights(filepath, overwrite=True)\n",
    "                        else:\n",
    "                            autoencoder.save(filepath, overwrite=True)\n",
    "                    else:\n",
    "                        if self.verbose > 0:\n",
    "                            print('Epoch %05d: %s did not improve' %\n",
    "                                  (epoch + 1, self.monitor))\n",
    "            else:\n",
    "                if self.verbose > 0:\n",
    "                    print('Epoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
    "                if self.save_weights_only:\n",
    "                    autoencoder.save_weights(filepath, overwrite=True)\n",
    "                else:\n",
    "                    autoencoder.save(filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_DNN_input = Input(shape=(original_dim,), name='Input')\n",
    "hidden_1 = Dense(intermediate_dim, activation=act_fun, name='Encoder_h1')\n",
    "aux = hidden_1(x_DNN_input)\n",
    "\n",
    "hidden_2 = Dense(intermediate_dim, activation=act_fun, name='Encoder_h2')\n",
    "\n",
    "aux = hidden_2(aux)\n",
    "\n",
    "L_z_mean = Dense(latent_dim, name='Latent_mean')\n",
    "T_z_mean = L_z_mean(aux)\n",
    "L_z_sigma_preActivation = Dense(latent_dim, name='Latent_sigma_h')\n",
    "\n",
    "aux = L_z_sigma_preActivation(aux)\n",
    "L_z_sigma = Lambda(InverseSquareRootLinearUnit, name='Latent_sigma')\n",
    "T_z_sigma = L_z_sigma(aux)\n",
    "\n",
    "L_z_latent = Lambda(sampling, name='Latent_sampling')([T_z_mean, T_z_sigma])\n",
    "decoder_h1 = Dense(intermediate_dim,\n",
    "                   activation=act_fun,\n",
    "                   kernel_constraint=max_norm(kernel_max_norm),\n",
    "                   name='Decoder_h1')(L_z_latent)\n",
    "\n",
    "decoder_h2 = Dense(intermediate_dim, activation=act_fun, name='Decoder_h2')(decoder_h1)\n",
    "\n",
    "L_par1 = Dense(original_dim, name='Output_par1')(decoder_h2)\n",
    "\n",
    "L_par2_preActivation = Dense(original_dim , name='par2_h')(decoder_h2)\n",
    "L_par2 = Lambda(InverseSquareRootLinearUnit, name='Output_par2')(L_par2_preActivation)\n",
    "\n",
    "L_par3_preActivation = Dense(Nf_lognorm, name='par3_h')(decoder_h2)\n",
    "L_par3 = Lambda(ClippedTanh, name='Output_par3')(L_par3_preActivation)\n",
    "\n",
    "fixed_input = Lambda(SmashTo0)(x_DNN_input)\n",
    "h1_prior = Dense(1,\n",
    "                 kernel_initializer='zeros',\n",
    "                 bias_initializer='ones',\n",
    "                 trainable=False,\n",
    "                 name='h1_prior'\n",
    "                )(fixed_input)\n",
    "\n",
    "L_prior_mean = Dense(latent_dim,\n",
    "                     kernel_initializer='zeros',\n",
    "                     bias_initializer='zeros',\n",
    "                     trainable=True,\n",
    "                     name='L_prior_mean'\n",
    "                    )(h1_prior)\n",
    "\n",
    "L_prior_sigma_preActivation = Dense(latent_dim,\n",
    "                                    kernel_initializer='zeros',\n",
    "                                    bias_initializer='ones',\n",
    "                                    trainable=True,\n",
    "                                    name='L_prior_sigma_preAct'\n",
    "                                   )(h1_prior)\n",
    "L_prior_sigma = Lambda(InverseSquareRootLinearUnit, name='L_prior_sigma')(L_prior_sigma_preActivation)\n",
    "\n",
    "params = KL.concatenate([T_z_mean, T_z_sigma, L_prior_mean, L_prior_sigma, L_par1, L_par2, L_par3], axis=1)\n",
    "\n",
    "L_RecoProb_1 = CustomIndividualLogNorLayer_1(name='RecoNLL_met')([x_DNN_input,L_par1,\n",
    "                                                              L_par2,L_par3])\n",
    "\n",
    "L_RecoProb_2 = CustomIndividualLogNorLayer_2(name='RecoNLL_mt')([x_DNN_input,L_par1,\n",
    "                                                              L_par2,L_par3])\n",
    "\n",
    "L_RecoProb_3 = CustomIndividualLogNorLayer_3(name='RecoNLL_mbb')([x_DNN_input,L_par1,\n",
    "                                                              L_par2,L_par3])\n",
    "\n",
    "L_RecoProb_4 = CustomIndividualLogNorLayer_4(name='RecoNLL_mct2')([x_DNN_input,L_par1,\n",
    "                                                              L_par2,L_par3])\n",
    "\n",
    "L_RecoProb_5 = CustomIndividualLogNorLayer_5(name='RecoNLL_mlb1')([x_DNN_input,L_par1,\n",
    "                                                              L_par2,L_par3])\n",
    "\n",
    "L_RecoProb_6 = CustomIndividualLogNorLayer_6(name='RecoNLL_lep1Pt')([x_DNN_input,L_par1,\n",
    "                                                               L_par2,L_par3])\n",
    "\n",
    "L_RecoProb_7 = CustomIndividualTruGauLayer_7(name='RecoNLL_nJet30')([x_DNN_input,L_par1,\n",
    "                                                              L_par2])\n",
    "\n",
    "L_RecoProb_8 = CustomIndividualTruGauLayer_8(name='RecoNLL_nBJet30_MV2c10')([x_DNN_input,L_par1,\n",
    "                                                              L_par2])\n",
    "\n",
    "\n",
    "L_RecoProb = CustomRecoProbLayer(name='RecoNLL')([x_DNN_input, L_par1, L_par2, L_par3])\n",
    "L_KLLoss = CustomKLLossLayer(name='KL')([T_z_mean, T_z_sigma, L_prior_mean, L_prior_sigma])\n",
    "vae = Model(inputs=x_DNN_input, outputs=[L_KLLoss, L_RecoProb,\n",
    "                                        L_RecoProb_1, L_RecoProb_2,\n",
    "                                        L_RecoProb_3, L_RecoProb_4, L_RecoProb_5,\n",
    "                                        L_RecoProb_6, L_RecoProb_7, L_RecoProb_8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def metric_wapper(layer):\n",
    "\n",
    "#     N = 4*latent_dim\n",
    "\n",
    "#     par1 = layer[:, N: N+original_dim]\n",
    "#     N += original_dim\n",
    "\n",
    "#     par2 = layer[:, N: N+original_dim]\n",
    "#     N += original_dim\n",
    "\n",
    "#     par3 = layer[:, N:N+Nf_lognorm]\n",
    "\n",
    "#     mu = layer[:, :latent_dim]\n",
    "#     sigma = layer[:, latent_dim: 2*latent_dim]\n",
    "#     mu_prior = layer[:, 2*latent_dim: 3*latent_dim]\n",
    "#     sigma_prior = layer[:, 3*latent_dim: 4*latent_dim]\n",
    "\n",
    "#     def metric(y_true, y_pred):\n",
    "\n",
    "#         KL = weight_KL_loss*KL_loss_forVAE(mu,sigma,mu_prior,sigma_prior)\n",
    "#         RecoLoss = RecoProb_forVAE(y_true[:, :original_dim], par1, par2, par3, w=1)\n",
    "\n",
    "#         return (K.mean(RecoLoss+KL))\n",
    "\n",
    "#     return metric\n",
    "\n",
    "# metric = metric_wapper(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roberto/anaconda3/envs/exotica_ray/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36564 samples, validate on 36563 samples\n",
      "Epoch 1/2000\n",
      "36564/36564 [==============================] - 7s 188us/step - loss: 25580.2709 - KL_loss: 1740.1951 - RecoNLL_loss: 24536.1532 - RecoNLL_met_loss: 3704.1694 - RecoNLL_mt_loss: 10293.3223 - RecoNLL_mbb_loss: 5278.9285 - RecoNLL_mct2_loss: 4392.8918 - RecoNLL_mlb1_loss: 847.8964 - RecoNLL_lep1Pt_loss: 5.9309 - RecoNLL_nJet30_loss: 1.9800 - RecoNLL_nBJet30_MV2c10_loss: 11.0328 - val_loss: 58.5986 - val_KL_loss: 23.6797 - val_RecoNLL_loss: 44.3907 - val_RecoNLL_met_loss: 6.9015 - val_RecoNLL_mt_loss: 5.4512 - val_RecoNLL_mbb_loss: 11.2738 - val_RecoNLL_mct2_loss: 6.3487 - val_RecoNLL_mlb1_loss: 6.8040 - val_RecoNLL_lep1Pt_loss: 5.2506 - val_RecoNLL_nJet30_loss: 1.2544 - val_RecoNLL_nBJet30_MV2c10_loss: 1.1066\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 58.59857, saving model to model_results/bump_single_train/bump_feat_0_1_2_3_4_5_6_7_ld_4/vae_bump_feat_0_1_2_3_4_5_6_7_ld_4.h5\n",
      "Epoch 00001: val_loss improved from inf to 58.59857, saving model to model_results/bump_single_train/bump_feat_0_1_2_3_4_5_6_7_ld_4/autoencoder_bump_feat_0_1_2_3_4_5_6_7_ld_4.h5\n",
      "Epoch 2/2000\n",
      "36564/36564 [==============================] - 2s 66us/step - loss: 52.0403 - KL_loss: 19.1999 - RecoNLL_loss: 40.5203 - RecoNLL_met_loss: 6.3427 - RecoNLL_mt_loss: 4.9299 - RecoNLL_mbb_loss: 10.5201 - RecoNLL_mct2_loss: 5.9596 - RecoNLL_mlb1_loss: 6.3058 - RecoNLL_lep1Pt_loss: 4.6199 - RecoNLL_nJet30_loss: 1.0570 - RecoNLL_nBJet30_MV2c10_loss: 0.7853 - val_loss: 47.8927 - val_KL_loss: 16.4069 - val_RecoNLL_loss: 38.0486 - val_RecoNLL_met_loss: 5.9951 - val_RecoNLL_mt_loss: 4.5917 - val_RecoNLL_mbb_loss: 9.9124 - val_RecoNLL_mct2_loss: 5.7257 - val_RecoNLL_mlb1_loss: 5.9299 - val_RecoNLL_lep1Pt_loss: 4.3574 - val_RecoNLL_nJet30_loss: 0.9098 - val_RecoNLL_nBJet30_MV2c10_loss: 0.6264\n",
      "\n",
      "Epoch 00002: val_loss improved from 58.59857 to 47.89270, saving model to model_results/bump_single_train/bump_feat_0_1_2_3_4_5_6_7_ld_4/vae_bump_feat_0_1_2_3_4_5_6_7_ld_4.h5\n",
      "Epoch 00002: val_loss improved from 58.59857 to 47.89270, saving model to model_results/bump_single_train/bump_feat_0_1_2_3_4_5_6_7_ld_4/autoencoder_bump_feat_0_1_2_3_4_5_6_7_ld_4.h5\n",
      "Epoch 3/2000\n",
      "36564/36564 [==============================] - 2s 68us/step - loss: 49.2028 - KL_loss: 17.7352 - RecoNLL_loss: 38.5617 - RecoNLL_met_loss: 6.0251 - RecoNLL_mt_loss: 4.6214 - RecoNLL_mbb_loss: 10.2210 - RecoNLL_mct2_loss: 5.7073 - RecoNLL_mlb1_loss: 5.9764 - RecoNLL_lep1Pt_loss: 4.4127 - RecoNLL_nJet30_loss: 0.9490 - RecoNLL_nBJet30_MV2c10_loss: 0.6488 - val_loss: 44.5661 - val_KL_loss: 15.2413 - val_RecoNLL_loss: 35.4213 - val_RecoNLL_met_loss: 5.5496 - val_RecoNLL_mt_loss: 4.2071 - val_RecoNLL_mbb_loss: 9.4113 - val_RecoNLL_mct2_loss: 5.2193 - val_RecoNLL_mlb1_loss: 5.5876 - val_RecoNLL_lep1Pt_loss: 4.1334 - val_RecoNLL_nJet30_loss: 0.7873 - val_RecoNLL_nBJet30_MV2c10_loss: 0.5256\n",
      "\n",
      "Epoch 00003: val_loss improved from 47.89270 to 44.56608, saving model to model_results/bump_single_train/bump_feat_0_1_2_3_4_5_6_7_ld_4/vae_bump_feat_0_1_2_3_4_5_6_7_ld_4.h5\n",
      "Epoch 00003: val_loss improved from 47.89270 to 44.56608, saving model to model_results/bump_single_train/bump_feat_0_1_2_3_4_5_6_7_ld_4/autoencoder_bump_feat_0_1_2_3_4_5_6_7_ld_4.h5\n",
      "Epoch 4/2000\n",
      "36564/36564 [==============================] - 3s 69us/step - loss: 44.3838 - KL_loss: 16.2560 - RecoNLL_loss: 34.6302 - RecoNLL_met_loss: 5.3709 - RecoNLL_mt_loss: 4.3352 - RecoNLL_mbb_loss: 8.6018 - RecoNLL_mct2_loss: 5.0977 - RecoNLL_mlb1_loss: 5.5845 - RecoNLL_lep1Pt_loss: 4.2680 - RecoNLL_nJet30_loss: 0.7744 - RecoNLL_nBJet30_MV2c10_loss: 0.5976 - val_loss: 39.5255 - val_KL_loss: 13.2670 - val_RecoNLL_loss: 31.5653 - val_RecoNLL_met_loss: 4.7402 - val_RecoNLL_mt_loss: 3.7890 - val_RecoNLL_mbb_loss: 7.1642 - val_RecoNLL_mct2_loss: 4.7380 - val_RecoNLL_mlb1_loss: 5.7137 - val_RecoNLL_lep1Pt_loss: 4.1428 - val_RecoNLL_nJet30_loss: 0.7403 - val_RecoNLL_nBJet30_MV2c10_loss: 0.5370\n",
      "\n",
      "Epoch 00004: val_loss improved from 44.56608 to 39.52551, saving model to model_results/bump_single_train/bump_feat_0_1_2_3_4_5_6_7_ld_4/vae_bump_feat_0_1_2_3_4_5_6_7_ld_4.h5\n",
      "Epoch 00004: val_loss improved from 44.56608 to 39.52551, saving model to model_results/bump_single_train/bump_feat_0_1_2_3_4_5_6_7_ld_4/autoencoder_bump_feat_0_1_2_3_4_5_6_7_ld_4.h5\n",
      "Epoch 5/2000\n",
      "36564/36564 [==============================] - 3s 70us/step - loss: 60.8529 - KL_loss: 36.5799 - RecoNLL_loss: 38.9050 - RecoNLL_met_loss: 5.7208 - RecoNLL_mt_loss: 4.7070 - RecoNLL_mbb_loss: 10.5489 - RecoNLL_mct2_loss: 5.5313 - RecoNLL_mlb1_loss: 5.9969 - RecoNLL_lep1Pt_loss: 4.6018 - RecoNLL_nJet30_loss: 0.8671 - RecoNLL_nBJet30_MV2c10_loss: 0.9311 - val_loss: 40.9638 - val_KL_loss: 14.5174 - val_RecoNLL_loss: 32.2534 - val_RecoNLL_met_loss: 4.9474 - val_RecoNLL_mt_loss: 4.2642 - val_RecoNLL_mbb_loss: 7.1490 - val_RecoNLL_mct2_loss: 5.1344 - val_RecoNLL_mlb1_loss: 5.3253 - val_RecoNLL_lep1Pt_loss: 4.1357 - val_RecoNLL_nJet30_loss: 0.7204 - val_RecoNLL_nBJet30_MV2c10_loss: 0.5770\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 39.52551\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/2000\n",
      "36564/36564 [==============================] - 3s 83us/step - loss: 41.4437 - KL_loss: 16.3710 - RecoNLL_loss: 31.6211 - RecoNLL_met_loss: 4.7954 - RecoNLL_mt_loss: 4.0968 - RecoNLL_mbb_loss: 6.9133 - RecoNLL_mct2_loss: 5.1409 - RecoNLL_mlb1_loss: 5.2355 - RecoNLL_lep1Pt_loss: 4.1399 - RecoNLL_nJet30_loss: 0.7298 - RecoNLL_nBJet30_MV2c10_loss: 0.5695 - val_loss: 39.5287 - val_KL_loss: 14.0034 - val_RecoNLL_loss: 31.1266 - val_RecoNLL_met_loss: 4.6586 - val_RecoNLL_mt_loss: 4.0276 - val_RecoNLL_mbb_loss: 6.8400 - val_RecoNLL_mct2_loss: 5.1409 - val_RecoNLL_mlb1_loss: 5.1671 - val_RecoNLL_lep1Pt_loss: 4.0580 - val_RecoNLL_nJet30_loss: 0.7014 - val_RecoNLL_nBJet30_MV2c10_loss: 0.5331\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 39.52551\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/2000\n",
      "36564/36564 [==============================] - 3s 71us/step - loss: 38.6291 - KL_loss: 14.5930 - RecoNLL_loss: 29.8733 - RecoNLL_met_loss: 4.4962 - RecoNLL_mt_loss: 3.7977 - RecoNLL_mbb_loss: 6.3990 - RecoNLL_mct2_loss: 5.0276 - RecoNLL_mlb1_loss: 5.0538 - RecoNLL_lep1Pt_loss: 3.8880 - RecoNLL_nJet30_loss: 0.6978 - RecoNLL_nBJet30_MV2c10_loss: 0.5133 - val_loss: 44.4342 - val_KL_loss: 18.9526 - val_RecoNLL_loss: 33.0626 - val_RecoNLL_met_loss: 5.0410 - val_RecoNLL_mt_loss: 4.3049 - val_RecoNLL_mbb_loss: 7.5939 - val_RecoNLL_mct2_loss: 5.4454 - val_RecoNLL_mlb1_loss: 5.1877 - val_RecoNLL_lep1Pt_loss: 4.0899 - val_RecoNLL_nJet30_loss: 0.7752 - val_RecoNLL_nBJet30_MV2c10_loss: 0.6245\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 39.52551\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/2000\n",
      " 3800/36564 [==>...........................] - ETA: 2s - loss: 42.7977 - KL_loss: 18.4418 - RecoNLL_loss: 31.7326 - RecoNLL_met_loss: 4.8503 - RecoNLL_mt_loss: 4.0202 - RecoNLL_mbb_loss: 7.1639 - RecoNLL_mct2_loss: 5.3260 - RecoNLL_mlb1_loss: 5.1616 - RecoNLL_lep1Pt_loss: 3.8789 - RecoNLL_nJet30_loss: 0.7650 - RecoNLL_nBJet30_MV2c10_loss: 0.5667"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "rn.seed(12345)\n",
    "\n",
    "adam = optimizers.adam(lr)\n",
    "\n",
    "vae.compile(optimizer=adam,\n",
    "            loss=[IdentityLoss, IdentityLoss,\n",
    "                  IdentityLoss, IdentityLoss,IdentityLoss,IdentityLoss,\n",
    "                 IdentityLoss,IdentityLoss,IdentityLoss,IdentityLoss],\n",
    "            loss_weights=[weight_KL_loss, 1.,\n",
    "                          0, 0, 0, 0,\n",
    "                         0, 0, 0, 0]\n",
    "#                     , metrics=[metric]\n",
    "           )\n",
    "\n",
    "\n",
    "train_history = vae.fit(x=train, y=[train, train,\n",
    "                                    train, train, train, train, train, train, train, train],\n",
    "    validation_data = (val, [val, val,\n",
    "                             val, val, val, val, val, val, val, val]),\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "\n",
    "    batch_size=batch_size,\n",
    "    callbacks =\n",
    "                [TerminateOnNaN(),\n",
    "                ModelCheckpoint(model_results_bump_single + '{}/vae_{}.h5'.format(name, name),\n",
    "                                    monitor='val_loss',\n",
    "                                    mode='auto', save_best_only=True,verbose=1,\n",
    "                                    period=1),\n",
    "                    EarlyStopping(monitor='val_loss', patience=50, verbose=1, min_delta=0.3),\n",
    "                    ReduceLROnPlateau(monitor='val_loss',\n",
    "                                      factor=0.8,\n",
    "                                      patience=5,\n",
    "                                      mode = 'auto',\n",
    "                                      epsilon=0.01,\n",
    "                                      cooldown=0,\n",
    "                                      min_lr=9e-8,\n",
    "                                      verbose=1),\n",
    "                SaveAutoencoder(model_results_bump_single +'{}/autoencoder_{}.h5'.format(name, name),\n",
    "                                    monitor='val_loss',\n",
    "                                    mode='auto', save_best_only=True,verbose=1,\n",
    "                                    period=1)\n",
    "                                    ])\n",
    "\n",
    "hist_df = pd.DataFrame(train_history.history)\n",
    "\n",
    "# or save to csv:\n",
    "hist_csv_file = model_results_bump_single +'{}/history_{}.csv'.format(name, name)\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
